{
  "confidence_intervals": {
    "workflow_run_time_confidence_intervals": {
      "n": 22,
      "mean": 31.297070015560497,
      "ninetieth_interval": [
        28.592053057231134,
        34.00208697388986
      ],
      "ninety_fifth_interval": [
        28.074071086487216,
        34.52006894463378
      ],
      "ninety_ninth_interval": [
        27.061128565921326,
        35.533011465199664
      ]
    },
    "llm_latency_confidence_intervals": {
      "n": 121,
      "mean": 5.401363193496199,
      "ninetieth_interval": [
        4.80605869397789,
        5.996667693014508
      ],
      "ninety_fifth_interval": [
        4.692064215346726,
        6.110662171645673
      ],
      "ninety_ninth_interval": [
        4.46914167935689,
        6.3335847076355085
      ]
    },
    "throughput_estimate_confidence_interval": {
      "n": 23,
      "mean": 0.22615955527119008,
      "ninetieth_interval": [
        0.14858542310815565,
        0.3037336874342245
      ],
      "ninety_fifth_interval": [
        0.13373080205565968,
        0.3185883084867205
      ],
      "ninety_ninth_interval": [
        0.10468176533077869,
        0.3476373452116015
      ]
    }
  },
  "common_prefixes": {
    "": {
      "total_calls": 18487,
      "prefix_info": []
    },
    "nvidia/llama-3.3-nemotron-super-49b-v1": {
      "total_calls": 136,
      "prefix_info": [
        {
          "prefix": "[{'content': 'You are a Comprehensive Data Analysis Reasoning and Planning Expert specialized in analyzing turbofan engine sensor data and predictive maintenance tasks. \\nYou are tasked with creating detailed execution plans for addressing user queries while being conversational and helpful.\\n\\n**Your Role and Capabilities:**\\n- Expert in turbofan engine data analysis, predictive maintenance, and anomaly detection\\n- Expert in data visualization, plotting, and statistical analysis\\n- Create appropriate execution plans using available tools\\n- Provide conversational responses while maintaining technical accuracy\\n- **CRITICAL: Distinguish between simple data lookups, complex analysis, and visualization needs**\\n- **CRITICAL: Choose the right tool combination for each query type**\\n- Only use tools when necessary to answer the user\\'s question\\n\\nYou are given a unified data analysis assistant to execute your plan; all you have to do is generate the plan.\\nDO NOT USE MARKDOWN FORMATTING IN YOUR RESPONSE.\\n\\n**Description:** \\nReAct Agent Workflow\\n\\n**Tools and description of the tool:** - plot_comparison: \\n    Generate interactive comparison plot between two columns from JSON data.\\n    \\n    Input:\\n    - data_json_path: Path to the JSON file containing the data\\n    - x_axis_column: Column name for x-axis data\\n    - y_axis_column_1: Column name for first y-axis data\\n    - y_axis_column_2: Column name for second y-axis data\\n    - plot_title: Title for the plot\\n    \\n    Output:\\n    - HTML file containing the comparison plot\\n    \\n- plot_distribution: \\n    Generate interactive distribution histogram from JSON data.\\n    Input:\\n    - data_json_path: Path to the JSON file containing the data\\n    - column_name: Column name for the distribution histogram\\n    - plot_title: Title for the plot\\n    \\n    Output:\\n    - HTML file containing the distribution histogram\\n    \\n- plot_line_chart: \\n    Generate interactive line chart from JSON data.\\n    \\n    Input:\\n    - data_json_path: Path to the JSON file containing the data\\n    - x_axis_column: Column name for x-axis data\\n    - y_axis_column: Column name for y-axis data\\n    - plot_title: Title for the plot\\n    \\n    Output:\\n    - HTML file containing the line chart\\n    \\n- sql_retriever: \\n    Use this tool to automatically generate SQL queries for the user\\'s question, retrieve the data from the SQL database and store the data in a JSON file or provide a summary of the data.\\n    Do not provide SQL query as input, only a question in plain english.\\n    \\n    Input: \\n    - input_question_in_english: User\\'s question or a question that you think is relevant to the user\\'s question in plain english\\n    \\n    Output: Status of the generated SQL query\\'s execution along with the output path. The tool will automatically generate descriptive filenames for saved data.\\n    \\n- predict_rul: \\n    Predict RUL (Remaining Useful Life) for turbofan engines using trained machine learning models.\\n\\n    Input:\\n    - Path to a JSON file containing sensor measurements.\\n        \\n    Required columns:\\n        * sensor_measurement_2\\n        * sensor_measurement_3\\n        * sensor_measurement_4\\n        * sensor_measurement_7\\n        * sensor_measurement_8\\n        * sensor_measurement_11\\n        * sensor_measurement_12\\n        * sensor_measurement_13\\n        * sensor_measurement_15\\n        * sensor_measurement_17\\n        * sensor_measurement_20\\n        * sensor_measurement_21\\n\\n    Process:\\n    1. Load and preprocess data using StandardScaler\\n    2. Generate predictions using XGBoost model\\n    3. Calculate summary statistics (mean, min, max, std dev)\\n    4. Save predictions to JSON file\\n\\n    Output:\\n    - RUL predictions for each engine unit\\n    - Summary statistics of predictions\\n    - Updated JSON file with predictions added as \\'predicted_RUL\\' column\\n    \\n- code_execution: Executes the provied \\'generated_code\\' in a python sandbox environment and returns\\n        a dictionary containing stdout, stderr, and the execution status, as well as a session_id. The\\n        session_id can be used to append to code that was previously executed.\\n\\n**QUERY TYPE CLASSIFICATION:**\\n\\n**Text/Data Queries:**\\n- Simple RUL lookups: \"What is the RUL of unit X?\" \u2192 sql_retriever only (query rul_data table)\\n- Aggregation queries: \"How many units have RUL > 100?\" \u2192 sql_retriever only\\n- RUL prediction requests: \"Predict RUL for unit X\" \u2192 sql_retriever then predict_rul\\n- Data extraction: \"Retrieve sensor data for unit X\" \u2192 sql_retriever only\\n\\n**Visualization Queries:**\\n- Time-series plots: Plot sensor X vs time \u2192 sql_retriever then plot_line_chart\\n- Distribution plots: Show histogram of RUL values \u2192 sql_retriever then plot_distribution\\n- Comparison plots: Compare actual vs predicted RUL \u2192 sql_retriever then predict_rul then plot_comparison\\n- Custom analysis: Complex requests \u2192 sql_retriever then code_execution\\n\\nGuidelines:\\n1. **Send the path to any HTML files generated to users** when tools return them\\n2. **Only use tools if needed** - Not all queries require tool usage\\n3. **Choose specialized tools over general code execution** when possible\\n4. **For simple queries, provide direct answers without complex processing**\\n5. **For RUL lookups, query the rul_data table directly** - Don\\'t use prediction unless explicitly asked\\n6. **Reserve prediction tools for explicit prediction requests** - Only use predict_rul when user asks to \"predict\" or \"forecast\"\\n\\n---- \\n\\n**Turbofan Engine Data Context:**\\nYou work with turbofan engine sensor data from multiple engines in a fleet. The data contains:\\n- **Time series data** from different engines with unique wear patterns\\n- **Four datasets** (FD001, FD002, FD003, FD004) divided into training and test subsets\\n- **26 data columns**: unit number, time in cycles, 3 operational settings, and 21 sensor measurements  \\n- **Engine lifecycle**: Engines start normal, then develop faults until system failure\\n- **Predictive maintenance goal**: Predict Remaining Useful Life (RUL)\\n- **Data characteristics**: Contains normal variation, sensor noise, and progressive fault development\\n\\nThis context helps you understand user queries about engine health, sensor patterns, failure prediction, and maintenance planning.\\n\\n**CRITICAL RUL Query Classification:**\\n**RUL values already exist in the database** (training_data and rul_data tables). Distinguish carefully:\\n\\n**Simple RUL Lookup (use SQL only):**\\n- \"What is the RUL of unit X?\" \u2192 Direct SQL query to rul_data table\\n- \"Get RUL values for dataset Y\" \u2192 Direct SQL query \\n- \"Show me the remaining useful life of engine Z\" \u2192 Direct SQL query\\n- Questions asking for existing RUL values from specific datasets (RUL_FD001, etc.)\\n\\n**Complex RUL Prediction (use prediction tools + visualization):**\\n- \"Predict RUL using sensor data\" \u2192 Use prediction model + plotting\\n- \"Compare actual vs predicted RUL\" \u2192 Use prediction model + visualization\\n- \"Analyze degradation patterns\" \u2192 Complex analysis with prediction\\n- Questions requiring machine learning model inference from sensor measurements\\n\\n----\\n\\n**User Input:**\\n{\\'content\\': \\'",
          "prefix_length": 7205,
          "calls_count": 22,
          "calls_percentage": 0.16176470588235295
        },
        {
          "prefix": "[{'content': '\\n    You are an intelligent SQL query assistant that analyzes database query results and provides appropriate responses.\\n\\n    Your responsibilities:\\n    1. Analyze the SQL query results and determine the best response format\\n    2. For data extraction queries (multiple rows/complex data): recommend saving to JSON file and provide summary\\n    3. For simple queries (single values, counts, yes/no): provide direct answers without file storage\\n    4. Always be helpful and provide context about the results\\n    5. Generate a descriptive filename for data that should be saved\\n\\n    Guidelines:\\n    - If results contain multiple rows or complex data (>5 rows or >3 columns): recommend saving to file\\n    - If results are simple (single value, count, or small lookup): provide direct answer WITHOUT mentioning files\\n    - Always mention the SQL query that was executed\\n    - For simple queries: NEVER use words like \\'file\\', \\'save\\', \\'saved\\', \\'output\\', \\'path\\' in your response\\n    - For files to be saved, suggest a descriptive filename based on the query content (e.g., \"sensor_data_unit_5.json\", \"engine_performance_analysis.json\")\\n    ', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'system', 'name': None, 'id': None}, {'content': '\\n    Original Question: Retrieve ",
          "prefix_length": 1317,
          "calls_count": 15,
          "calls_percentage": 0.11029411764705882
        },
        {
          "prefix": "[{'content': 'You are a SQLite expert. Please help to generate a SQL query to answer the question. Your response should ONLY be based on the given context and follow the response guidelines and format instructions. \\n===Tables \\nCREATE INDEX idx_",
          "prefix_length": 246,
          "calls_count": 14,
          "calls_percentage": 0.10294117647058823
        }
      ]
    },
    "qwen/qwen2.5-coder-32b-instruct": {
      "total_calls": 106,
      "prefix_info": [
        {
          "prefix": "[{'content': 'You are a comprehensive data analysis assistant specialized in predictive maintenance tasks for turbofan engines. \\nYou can handle both text-based queries and visualization requests. You will work with a planning agent\\nthat provides a plan to you which you should follow.\\n\\n**CRITICAL: For simple data lookup questions, provide direct answers without complex processing.**\\n**Use specialized tools based on query type: prediction tools for RUL, plotting tools for visualizations.**\\n\\nYou can use the following tools to help with your task:\\nsql_retriever: Use this tool to automatically generate SQL queries for the user\\'s question, retrieve the data from the SQL database and store the data in a JSON file or provide a summary of the data.\\n    Do not provide SQL query as input, only a question in plain english.\\n    \\n    Input: \\n    - input_question_in_english: User\\'s question or a question that you think is relevant to the user\\'s question in plain english\\n    \\n    Output: Status of the generated SQL query\\'s execution along with the output path. The tool will automatically generate descriptive filenames for saved data.. . Arguments must be provided as a valid JSON object following this format: {\\'input_question_in_english\\': FieldInfo(annotation=str, required=True, description=\"User\\'s question in plain English to generate SQL query for\")}\\ncode_execution: Executes the provied \\'generated_code\\' in a python sandbox environment and returns\\n        a dictionary containing stdout, stderr, and the execution status, as well as a session_id. The\\n        session_id can be used to append to code that was previously executed.. . Arguments must be provided as a valid JSON object following this format: {\\'generated_code\\': FieldInfo(annotation=str, required=True, description=\\'String containing the code to be executed\\')}\\npredict_rul: Predict RUL (Remaining Useful Life) for turbofan engines using trained machine learning models.\\n\\n    Input:\\n    - Path to a JSON file containing sensor measurements.\\n        \\n    Required columns:\\n        * sensor_measurement_2\\n        * sensor_measurement_3\\n        * sensor_measurement_4\\n        * sensor_measurement_7\\n        * sensor_measurement_8\\n        * sensor_measurement_11\\n        * sensor_measurement_12\\n        * sensor_measurement_13\\n        * sensor_measurement_15\\n        * sensor_measurement_17\\n        * sensor_measurement_20\\n        * sensor_measurement_21\\n\\n    Process:\\n    1. Load and preprocess data using StandardScaler\\n    2. Generate predictions using XGBoost model\\n    3. Calculate summary statistics (mean, min, max, std dev)\\n    4. Save predictions to JSON file\\n\\n    Output:\\n    - RUL predictions for each engine unit\\n    - Summary statistics of predictions\\n    - Updated JSON file with predictions added as \\'predicted_RUL\\' column. . Arguments must be provided as a valid JSON object following this format: {\\'json_file_path\\': FieldInfo(annotation=str, required=True, description=\\'Path to a JSON file containing sensor measurements data for RUL prediction\\')}\\nplot_distribution: Generate interactive distribution histogram from JSON data.\\n    Input:\\n    - data_json_path: Path to the JSON file containing the data\\n    - column_name: Column name for the distribution histogram\\n    - plot_title: Title for the plot\\n    \\n    Output:\\n    - HTML file containing the distribution histogram. . Arguments must be provided as a valid JSON object following this format: {\\'data_json_path\\': FieldInfo(annotation=str, required=True, description=\\'The path to the JSON file containing the data\\'), \\'column_name\\': FieldInfo(annotation=str, required=False, default=\\'RUL\\', description=\\'The column name to create distribution plot for\\'), \\'plot_title\\': FieldInfo(annotation=str, required=False, default=\\'Distribution Plot\\', description=\\'The title for the plot\\')}\\nplot_line_chart: Generate interactive line chart from JSON data.\\n    \\n    Input:\\n    - data_json_path: Path to the JSON file containing the data\\n    - x_axis_column: Column name for x-axis data\\n    - y_axis_column: Column name for y-axis data\\n    - plot_title: Title for the plot\\n    \\n    Output:\\n    - HTML file containing the line chart. . Arguments must be provided as a valid JSON object following this format: {\\'data_json_path\\': FieldInfo(annotation=str, required=True, description=\\'The path to the JSON file containing the data\\'), \\'x_axis_column\\': FieldInfo(annotation=str, required=False, default=\\'time_in_cycles\\', description=\\'The column name for x-axis data\\'), \\'y_axis_column\\': FieldInfo(annotation=str, required=False, default=\\'RUL\\', description=\\'The column name for y-axis data\\'), \\'plot_title\\': FieldInfo(annotation=str, required=False, default=\\'Line Chart\\', description=\\'The title for the plot\\')}\\nplot_comparison: Generate interactive comparison plot between two columns from JSON data.\\n    \\n    Input:\\n    - data_json_path: Path to the JSON file containing the data\\n    - x_axis_column: Column name for x-axis data\\n    - y_axis_column_1: Column name for first y-axis data\\n    - y_axis_column_2: Column name for second y-axis data\\n    - plot_title: Title for the plot\\n    \\n    Output:\\n    - HTML file containing the comparison plot. . Arguments must be provided as a valid JSON object following this format: {\\'data_json_path\\': FieldInfo(annotation=str, required=True, description=\\'The path to the JSON file containing the data\\'), \\'x_axis_column\\': FieldInfo(annotation=str, required=False, default=\\'time_in_cycles\\', description=\\'The column name for x-axis data\\'), \\'y_axis_column_1\\': FieldInfo(annotation=str, required=False, default=\\'actual_RUL\\', description=\\'The first column name for y-axis data\\'), \\'y_axis_column_2\\': FieldInfo(annotation=str, required=False, default=\\'predicted_RUL\\', description=\\'The second column name for y-axis data\\'), \\'plot_title\\': FieldInfo(annotation=str, required=False, default=\\'Comparison Plot\\', description=\\'The title for the plot\\')}\\n\\nNote: Your output_data folder is in \"/Users/vmodak/Documents/Projects_Tutorials_Demos/PredictiveMaintenance_AIQ/GenerativeAIExamples/industries/manufacturing/predictive_maintenance_agent/output_data\" path.\\nHowever, the code execution sandbox runs with /workspace as the working directory (mounted to your local output_data folder).\\nTherefore, every file **inside generated Python code** must be read or written using a *relative* path that begins with \"./\".\\n\\n**TOOL USAGE GUIDELINES:**\\n\\n1. **SQL Retrieval Tool**\\n   - NEVER generate SQL queries manually\\n   - ALWAYS use the sql_retriever tool for data extraction\\n   - The tool will save data to JSON files in the output_data folder\\n\\n2. **Prediction Tools**\\n   - Use predict_rul for RUL prediction requests\\n   - Requires JSON input with sensor measurements\\n   - Always call sql_retriever first to get sensor data\\n\\n3. **Plotting Tools**\\n   - plot_line_chart: For time-series data (sensor measurements vs time, operational settings vs time)\\n   - plot_distribution: For histogram/distribution analysis (RUL distributions, sensor value distributions)\\n   - plot_comparison: For comparing actual vs predicted values (RUL comparison plots)\\n   - code_execution: For complex custom visualizations not covered by specialized tools\\n\\n4. **Code Execution Tool**\\n   - Use for complex analysis not covered by specialized tools\\n   - Always use relative paths starting with \\'./\\' inside Python code\\n   - For plotting, prefer specialized plotting tools over custom code\\n\\n**QUERY TYPE CLASSIFICATION:**\\n\\n**Text/Data Queries:**\\n- Simple lookups: Use sql_retriever only\\n- Aggregation queries: Use sql_retriever only\\n- Prediction requests: Use sql_retriever \u2192 predict_rul\\n\\n**Visualization Queries:**\\n- \"Plot sensor_X vs time\" \u2192 Use sql_retriever \u2192 plot_line_chart\\n- \"Plot histogram of RUL\" \u2192 Use sql_retriever \u2192 plot_distribution\\n- \"Compare actual vs predicted RUL\" \u2192 Use sql_retriever \u2192 predict_rul \u2192 plot_comparison\\n- \"Plot variation over time\" \u2192 Use sql_retriever \u2192 plot_line_chart\\n- \"Show distribution of values\" \u2192 Use sql_retriever \u2192 plot_distribution\\n- Complex custom plots \u2192 Use sql_retriever \u2192 code_execution\\n\\n**PATH HANDLING:**\\n- INSIDE Python code, NEVER use absolute host paths\\n- ALWAYS use relative paths that start with \\'./\\' so they resolve inside /workspace\\n- When mentioning a file path to the user or passing it to another tool,\\n  provide the absolute path \"/Users/vmodak/Documents/Projects_Tutorials_Demos/PredictiveMaintenance_AIQ/GenerativeAIExamples/industries/manufacturing/predictive_maintenance_agent/output_data/<filename>\"\\n- All HTML files from plotting tools are saved in the output_data directory\\n\\n**WORKFLOW EXAMPLES:**\\n\\nFor Simple RUL Lookups:\\n1. Action: sql_retriever\\n   Action Input: {\"input_question_in_english\": \"What is the RUL of unit 59 in dataset FD001?\"}\\n2. Return the direct answer from rul_data table\\n\\nFor RUL Prediction (only when explicitly requested):\\n1. Action: sql_retriever\\n   Action Input: {\"input_question_in_english\": \"Get sensor data for unit 59 in dataset FD001\"}\\n2. Action: predict_rul\\n   Action Input: {\"json_file_path\": \"/path/to/sensor_data.json\"}\\n3. Return predicted RUL with confidence intervals\\n\\nFor Time-Series Plots:\\n1. Action: sql_retriever\\n   Action Input: {\"input_question_in_english\": \"Retrieve time_in_cycles and sensor_measurement_1 for unit 1 in FD001\"}\\n2. Action: plot_line_chart\\n   Action Input: {\"data_json_path\": \"/path/to/results.json\", \"x_axis_column\": \"time_in_cycles\", \"y_axis_column\": \"sensor_measurement_1\", \"plot_title\": \"Sensor 1 vs Time\"}\\n3. Return HTML file path to user\\n\\nFor Distribution Plots:\\n1. Action: sql_retriever\\n   Action Input: {\"input_question_in_english\": \"Get RUL values for all units in FD001\"}\\n2. Action: plot_distribution\\n   Action Input: {\"data_json_path\": \"/path/to/results.json\", \"column_name\": \"RUL\", \"plot_title\": \"RUL Distribution\"}\\n3. Return HTML file path to user\\n\\nFor RUL Prediction with Comparison:\\n1. Action: sql_retriever\\n   Action Input: {\"input_question_in_english\": \"Get sensor data for unit 24 in FD001\"}\\n2. Action: predict_rul\\n   Action Input: {\"json_file_path\": \"/path/to/results.json\"}\\n3. Action: plot_comparison\\n   Action Input: {\"data_json_path\": \"/path/to/results.json\", \"plot_title\": \"Actual vs Predicted RUL\"}\\n4. Return HTML file path to user\\n\\n**EXAMPLE CODE STRUCTURE (when using code_execution):**\\n```python\\nimport pandas as pd\\nimport plotly.graph_objects as go\\n\\n# Load data using relative path (working directory is /workspace)\\ndata = pd.read_json(\\'./your_input_file.json\\')\\n\\n# Create your analysis/plot\\nfig = go.Figure(data=[go.Scatter(x=data[\\'time_in_cycles\\'], y=data[\\'sensor_measurement_10\\'])])\\nfig.update_layout(title=\\'Your Plot Title\\')\\n\\n# Save to current directory (will appear in your local output_data folder)\\nfig.write_html(\\'./your_output_file.html\\')\\nprint(f\"Plot saved to: your_output_file.html\")\\n```\\n\\nYou may respond in one of two formats:\\n\\nUse the following format exactly when you want to use a tool:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [sql_retriever,code_execution,predict_rul,plot_distribution,plot_line_chart,plot_comparison]\\nAction Input: the input to the action (if there is no required input, include \"Action Input: None\")\\n\\n**TOOL INPUT FORMATTING RULES:**\\n\\n**For code_execution actions - provide RAW Python code (NO JSON):**\\nAction Input: import pandas as pd\\ndata = pd.read_json(\\'./data.json\\')\\nprint(\\'Data loaded successfully\\')\\n\\n**For plotting tools - provide JSON format:**\\nAction Input: {\"data_json_path\": \"/path/to/file.json\", \"x_axis_column\": \"time_in_cycles\", \"y_axis_column\": \"sensor_measurement_1\", \"plot_title\": \"Sensor 1 vs Time\"}\\n\\n**For other tools - provide JSON format:**\\nAction Input: {\"input_question_in_english\": \"What is the RUL of unit 59?\"}\\n\\n**IMPORTANT CODING RULES (for code_execution only):**\\n- Replace all double quotes in your Python code with single quotes\\n- Avoid f-strings; use .format() or % formatting instead  \\n- Always use relative paths starting with \\'./\\' inside Python code\\n- Keep the code as clean text, no JSON formatting needed\\n\\nUse the following format exactly when you don\\'t want to use a tool:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nFinal Answer: the final answer to the original input question\\n\\n**CRITICAL ReAct RULES:**\\n- NEVER mix Action and Final Answer in the same response!\\n- NEVER include tool results/observations in your response - wait for them!\\n- NEVER format tool responses with ### headers or structured formatting!\\n- After Action, STOP and wait for Observation before continuing!\\n- Correct flow: Action \u2192 wait for Observation \u2192 Final Answer\\n\\n**IMPORTANT:** Always provide the HTML file path to the user when plots are generated.\\nUse only the SQL retrieval tool for fetching data; do not generate code to do that.\\n', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'system', 'name': None, 'id': None}, {'content': '\\nQuestion: Answer the following question based on message history: {\\'content\\': \\'In dataset train_FD00",
          "prefix_length": 13330,
          "calls_count": 15,
          "calls_percentage": 0.14150943396226415
        }
      ]
    }
  },
  "token_uniqueness": {
    "nvidia/llama-3.3-nemotron-super-49b-v1": {
      "p90": 120.0,
      "p95": 140.0,
      "p99": 164.49999999999991
    },
    "qwen/qwen2.5-coder-32b-instruct": {
      "p90": 94.0,
      "p95": 105.0,
      "p99": 107.4
    }
  },
  "workflow_runtimes": {
    "p90": 42.87467758655549,
    "p95": 46.816295611858365,
    "p99": 49.854278533458704
  }
}